# ----------------------------------------------------------------------------
# Inversions
# ----------------------------------------------------------------------------
# OPTIONAL: clear environment (comment out if you don't like this behaviour)
rm(list = ls())

library(SCOPEinR)
library(ToolsRTM)

if (!require("dplyr")) { install.packages("dplyr"); require("dplyr") }  ###
if (!require("readr")) { install.packages("readr"); require("readr") }  ###
if (!require("ggplot2")) { install.packages("ggplot2"); require("ggplot2") }  ###
if (!require("tidyr")) { install.packages("tidyr"); require("tidyr") }  ###
if (!require("fmsb")) { install.packages("fmsb"); require("fmsb") }  ###
if (!require("doParallel")) { install.packages("doParallel"); require("doParallel") }  ###
if (!require("parallel")) { install.packages("parallel"); require("parallel") }  ###
if (!require("caret")) { install.packages("caret"); require("caret") }  ###
if (!require("randomForest")) { install.packages("randomForest"); require("randomForest") }  ###
if (!require("pls")) { install.packages("pls"); require("pls") }  ###
if (!require("e1071")) { install.packages("e1071"); require("e1071") }  ###
if (!require("gbm")) { install.packages("gbm"); require("gbm") }  ###


if (!require("keras")) { install.packages("keras"); require("keras") }  ###
if (!require("tensorflow")) { install.packages("tensorflow"); require("tensorflow") }  ###
if (!require("reticulate")) { install.packages("reticulate"); require("reticulate") }  ###

source('codes/compute_stats.R')
# ----------------------------------------------------------------------------
# 0. Install python enviroment and tensorflow (ony one time)
# ----------------------------------------------------------------------------


#library(reticulate)
use_condaenv("r-tf", required = TRUE)

#library(tensorflow)

# 2) Instalar TensorFlow 2.15 en el entorno conda 'r-tf'
#tensorflow::install_tensorflow(
  #version = "2.15.0",
  #method  = "conda",
 # envname = "r-tf"
#)

# Check it
reticulate::py_config()

reticulate::py_run_string("
import tensorflow as tf
import keras
print('TF version:', tf.__version__)
print('Keras version:', keras.__version__)
")



# ----------------------------------------------------------------------------
# 1. Load LUT (inputs + traits) and apparent reflectance
# ----------------------------------------------------------------------------

# Get the last SCOPE output folder (most recent run)
path_out = 'outs/'
subdirs          <- list.dirs(path_out, recursive = FALSE)
last_subdir      <- subdirs[length(subdirs)]
cat("\nUsing folder for plots:", last_subdir, "\n")
path_run <- last_subdir   # <- adjust!


# Load LUT generated by SCOPE (these are the real inputs used)
LUT <- read.csv(file.path(path_run, "Parameters", "inputLUT.csv"))

# Apparent reflectance from SCOPE (top-of-canopy reflectance seen by a sensor)
# Each row = one simulation, columns X400...X2500 = wavelengths
reflapp <- read.csv(file.path(path_run, "reflapp.csv"))

# Keep only the first nrows that match LUT (or join by n.sim if needed)
stopifnot(nrow(LUT) == nrow(reflapp))   # simple check for teaching


# Load wavelength definitions from SCOPEinR
bands <- SCOPEinR::define.bands()
wlS   <- bands$wlS[1:2001]   # 400–2500 nm (shortwave)
wlF   <- bands$wlF           # 640–850 nm (fluorescence)

# Replace spectral column names with correct wavelengths
colnames(reflapp)[1:ncol(reflapp)] <- paste('R.',wlS,sep='')
reflapp[1:3,1:3]

# ----------------------------------------------------------------------------
# 2.Get indices
# ----------------------------------------------------------------------------


df.indices <-ToolsRTM::getIndices(reflapp, pattern.rfl = 'R.', spectral.domain = 'VNIR')
df.indices <- df.indices[, colSums(is.na(df.indices)) == 0]
head(df.indices)


# ----------------------------------------------------------------------------
# 3. Define target trait and predictor variables
# ----------------------------------------------------------------------------

target_trait <- "Cab"   # <-- choose the trait you want to retrieve

#Target wavelengths every 5 nm from 400 to 800
target_wavelengths <- paste('R.',seq(400, 800, by = 5),sep='')


# Combine all predictors you want to use
predictors <- c(target_wavelengths)

# Build the modelling dataset: 1 column for Cab + predictors
dataset <- cbind(
  Cab = LUT[[target_trait]],
  reflapp[, predictors],
  df.indices
)

# Remove any duplicated columns (can happen if names overlap)
dataset <- dataset[, !duplicated(names(dataset))]

head(dataset)



df_spec <- dataset %>%
  select(starts_with("R.")) %>%       # keep only spectral columns
  mutate(sim = 1:n()) %>%             # simulation index
  pivot_longer(cols = starts_with("R."),
               names_to = "Wavelength",
               values_to = "Reflectance")

# Convert "R.400" → 400
df_spec$Wavelength <- as.numeric(gsub("R\\.", "", df_spec$Wavelength))

ggplot(df_spec, aes(x = Wavelength, y = Reflectance, group = sim)) +
  geom_line(alpha = 0.25, color = "darkblue") +
  theme_bw() +
  xlab("Wavelength (nm)") +
  ylab("Reflectance") +
  ggtitle("Reflectance spectra for all simulations")



# ----------------------------------------------------------------------------
# 4. Optional: remove highly collinear predictors using VIF
# ----------------------------------------------------------------------------

set.seed(42)
rows.vif <- sample(nrow(dataset), min(50, nrow(dataset)))  # subset for speed

vif_keep <- ToolsRTM::getVIF(
  dataset[rows.vif, -1],   # all predictors (exclude first column = Cab)
  thresh = 10              # VIF threshold
)
print(vif_keep)
# Keep only predictors selected by VIF
dataset_vif <- cbind(Cab = dataset$Cab, dataset[, vif_keep])



# ----------------------------------------------------------------------------
# 5. Inversion using SVM &RF
# ----------------------------------------------------------------------------



model.svm=get.inversion(data = dataset_vif, depVar = "Cab", inputs = vif_keep, algorithm  = "SVM",
              n.samples = 80,seed = 123)



model.rf=get.inversion(data = dataset_vif, depVar = "Cab", inputs = vif_keep, algorithm  = "RF",
                    n.samples = 80,seed = 123)

plot(model.rf$importance)
model.rf$statistics

model.nn=get.inversion(data = dataset_vif, depVar = "Cab", inputs = vif_keep, algorithm  = "NN",
                       n.samples = 80,seed = 123)

model.gb=get.inversion(data = dataset_vif, depVar = "Cab", inputs = vif_keep, algorithm  = "GB",
                       n.samples = 80,seed = 123)




# ----------------------------------------------------------------------------
# 6. Inversion using RF from scrath
# ----------------------------------------------------------------------------

set.seed(123)
depVar <- 'Cab'
# split train / test
train.index <- caret::createDataPartition(dataset_vif$Cab, p = 0.7, list = FALSE)
df.train  <- dataset_vif[train.index, ]
df.test   <- dataset_vif[-train.index, ]
inputs <- vif_keep
#inputs <- predictors

# ------------------------------------------------------------------
# 1. Build formula: depVar ~ band1 + band2 + ...
# ------------------------------------------------------------------
fmla.n <- as.formula(
  paste(depVar, " ~ ", paste(inputs, collapse = "+"))
)
print(fmla.n)
# ------------------------------------------------------------------
# 2. Parallel backend for tuning and training
# ------------------------------------------------------------------
n.cores <- parallel::makeCluster(detectCores() - 2)


clusters <- parallel::makeCluster(n.cores)
doParallel::registerDoParallel(clusters)


# ------------------------------------------------------------------
# 3. Tune RF mtry using tuneRF (on training data only)
#    - df.train[, inputs] : predictor matrix
#    - df.train[, depVar] : response vector
# ------------------------------------------------------------------
mtry2 <- randomForest::tuneRF(
  x          = df.train[, inputs],
  y          = df.train[, depVar],
  ntreeTry   = ncol(df.train[, inputs]) / 3,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE,
  plot       = FALSE
)


# best mtry value = column where OOB error (2nd col) is minimum
best.m  <- mtry2[mtry2[, 2] == min(mtry2[, 2]), 1]
# ------------------------------------------------------------------
# 4. Train final RF model with caret::train
# ------------------------------------------------------------------
metric    <- "RMSE"
tunegrid  <- expand.grid(.mtry = best.m)
n.trees   <- round(ncol(df.train[, inputs]) / 3, 0)


method.resampling <- 'cv'
fit.control <- caret::trainControl(
  method         = method.resampling,  # e.g. "repeatedcv"
  number         = 3,
  repeats        = 3,
  search         = "grid",
  allowParallel  = TRUE
)

rf.model <- caret::train(
  fmla.n,
  data      = df.train,
  method    = "rf",
  metric    = metric,
  trControl = fit.control,
  tuneGrid  = tunegrid,
  verbose   = FALSE
)


print(rf.model)

# ------------------------------------------------------------------
# 5. Stop parallel cluster
# ------------------------------------------------------------------
parallel::stopCluster(clusters)

# ------------------------------------------------------------------
# 6. Variable importance from RF
# ------------------------------------------------------------------
importance <- caret::varImp(rf.model, scale = FALSE)
plot(importance)
# ------------------------------------------------------------------
# 7. Train/Test predictions and skill statistics
# ------------------------------------------------------------------
# df.train and df.test must both contain depVar + inputs
pred.train <- predict(rf.model, newdata = df.train)
pred.test  <- predict(rf.model, newdata = df.test)

stats.table <- compute_stats(
  pred.train, df.train$Cab,
  pred.test,  df.test$Cab
)

print(stats.table)


plot.result <- get.plot.ML(model = rf.model,
                           df.train[, c(depVar,inputs)],
                           df.test[, c(depVar, inputs)], depVar)




# ----------------------------------------------------------------------------
# 6. Inversion using deep ML
# ----------------------------------------------------------------------------
py_require("keras")
py_require('Tensorflow')
py_require_legacy_keras()

# ----------------------------------------------------------------------------
#	 6.1. Set parameters for Deep ML models  -----
# ----------------------------------------------------------------------------

keras::py_require_legacy_keras()
tensorflow::py_require_tensorflow()
reticulate::py_config()
# output folder
paths.models='Models/'
ifelse(!dir.exists(paths.models), dir.create(paths.models), FALSE)

# 2. At the very top of your script / R session:
library(keras)        # legacy keras (tf-keras, not keras3)
py_require_legacy_keras()   # <- THIS is the important line

# (optional, to check)
tensorflow::tf_config()

depVar<-c('Cab')

#depVar<-c('Vcmax25','F760','Actot')
method.preProcess = c('Normalize','Standarize','Center','YeoJohnson') ## options: 'Normalize', 'YeoJohnson','BoxCox', Standarize', 'Center','Scale', and 'PCA'

deepML <-c('Hidden-layers','CNN') ### options: 'CNN','Hidden-layers'
transformation ='preProcess' ## options: 'PCA','preProcess'
optimizer.ml ='adam' ### options: 'adam','adadelta','adagrad', 'adamax', 'nadam', 'msprop', 'sgd'

vif_keep

models.cab <-getMLmodel.withRetrain(
  dataset = dataset_vif[c(depVar,vif_keep)],
  depVar = depVar,
  model = "CNN",
  optimizer = "adam",
  n.times = 3,
  n.neurons = 128,
  n.layers = 4,
  batch.size = 125,
  n.epochs = 100,
  save.model = T,
  path.model = paths.models,
  prop.split = c(0.8, 0.2),
  data.trans = "preProcess",
  method.preProcess = "Normalize",
  depVar.trans = FALSE
)



